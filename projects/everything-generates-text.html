<!DOCTYPE html>
<html lang="en">
  <head>
    <link rel="icon" href="favicon.ico"> 
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=5.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<link rel="icon" href="favicon.ico"> 
<!-- I just can't get this favicon to load -->

    <meta name="author" content="Ronik Bhaskar">


    <meta name="subtitle" content="Maker & Student of Life">


    <meta name="description" content="In order to demonstrate that text generation is not special, I wanted to create something that shows how unspectacular generating text really is. And so, I present, Everything Generates Text.">


    <meta name="keywords" content="research, programming, computer science, math, art, writing, stories">




<title>Everything Generates Text | Ronik Bhaskar</title>



    <link rel="icon" href="/base_logo_black_white_circle.ico">
    <!-- <link rel="icon" href="favicon.ico?" type="image/x-icon"> -->



<style>
    @import url('https://fonts.googleapis.com/css2?family=Libre+Caslon+Text&display=swap');
    @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&family=Roboto+Mono&display=swap');
</style>



    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    







    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>






<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-YG0RQTX7GQ"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-YG0RQTX7GQ');
</script>
<!-- End Google Analytics -->




  <meta name="generator" content="Hexo 6.3.0"></head>
  <body>
    <div class="mask-border">
    </div>

    <div class="wrapper">

      <div class="header">
  <div class="flex-container">
    <div class="header-inner">
      <div class="site-brand-container">
        <a href="/">
          
            <img class="logo-img" src="/base_logo_black.png" alt="logo_image">
          
        </a>
      </div>
      <div id="menu-btn" class="menu-btn" onclick="toggleMenu()">
        <script>
            function toggleMenu() {
                var menuList = document.getElementsByClassName("menu-list")[0];
                var menuButton = document.getElementById("menu-btn");  
                if(menuList.classList.contains("active")){
                  menuList.classList.remove("active");
                  menuButton.innerHTML = "MENU";
                }else{
                  menuList.classList.add("active");
                  menuButton.innerHTML = "<div class=\"icon arrow-up\"> </div>";
                }
            }
        </script>
        Menu
      </div>
      <nav class="site-nav">
        <ul class="menu-list">
          
            
              <li class="menu-item">
                <a href="/">Home</a>
              </li> 
                   
          
            
              <li class="menu-item">
                <a href="/publications/">Publications</a>
              </li> 
                   
          
            
              <li class="menu-item">
                <a href="/projects/">Projects</a>
              </li> 
                   
          
            
              <li class="menu-item">
                <a href="/about/CV.pdf">CV</a>
              </li> 
                   
          
          
            <li class="menu-item search-btn">
              <a href="#">Search</a>
            </li>
          
        </ul>
      </nav>
    </div>
  </div>
</div>


      <div class="main">
        <div class="flex-container">
          <article id="post">

  
    <div class="post-head">
    <div class="post-info">
        <div class="post-title">
            
            
                Everything Generates Text
            
            
        </div>
    </div>
    <div class="h-line-primary"></div>
</div>
    <div class="post-content">
    <p>Because every ML model can be forced to generate text. And Jon Ma is the Best. And Ronik too, lol.</p>
<h2 id="Thanks-for-that-introduction-Jon-I’ll-take-it-from-here"><a href="#Thanks-for-that-introduction-Jon-I’ll-take-it-from-here" class="headerlink" title="Thanks for that introduction, Jon. I’ll take it from here."></a>Thanks for that introduction, Jon. I’ll take it from here.</h2><p>LLMs, such as ChatGPT, don’t understand language. They aren’t having real conversations with you. They can’t construct meaning from words. They know nothing.</p>
<p>So, how do they do what they do? Simple. They are next-word* predictors. </p>
<p>LLMs take whatever input text they are given, and they try to predict what the next word should be. Then, they add the predicted word to their input text and repeat the process. Inside the LLM, it looks something like this:</p>
<ol>
<li>Once -&gt; [upon, 57% probability]</li>
<li>Once upon -&gt; [a, 72% probability]</li>
<li>Once upon a -&gt; [time, 86% probability]</li>
</ol>
<p>And so on. This is called autoregressive prediction, and it relies on one, massive assumption about language: all the words that come before give you enough information to guess what word comes next.</p>
<p>So far, this assumption has worked pretty well, but there’s one problem. Because the software and hardware necessary to run LLMs is so incredibly complicated, companies and hucksters argue their machines are capable of actually thinking. Of reasoning. Of doing what human brains do. And this is simply false.</p>
<p>LLMs are no more capable of cognition than a shuffled deck of cards. Sure, sometimes you might see interesting patterns, but that’s only because you gave the patterns meaning, not because the LLMs are doing anything special. LLMs generate sequences of words. You give those words meaning.</p>
<p>In order to demonstrate that text generation is not special, I wanted to create something that shows how unspectacular generating text really is. And so, I present, Everything Generates Text.</p>
<h2 id="Premise"><a href="#Premise" class="headerlink" title="Premise"></a>Premise</h2><p>LLMs are very fancy, expensive machine learning (ML) models, but ML models don’t need to be fancy or expensive to generate text. In fact, I believe any ML model can be coerced into generating text, because it’s not the machine that’s special, it’s just about training on the right data.</p>
<h3 id="Drawing-Lines"><a href="#Drawing-Lines" class="headerlink" title="Drawing Lines"></a>Drawing Lines</h3><p>Let’s start with the most basic ML model: linear regression. You are absolutely familiar with linear regression. If you’ve ever seen a bunch of points scattered on a plot, and a trend line drawn through those points, congrats! You witnessed ML in action. Linear regression really is just drawing lines through data. So, if we can use this to generate text, then anything can generate text, right?</p>
<p>Well, it works. It’s not fantastic, but sometimes it makes sense. To make this work, I essentially turned all the words into points scattered on a plot. I also needed to change our assumptions. Earlier, I said LLMs assume that if you know all the words before, then you can predict the next word. As it turns out, all the words requires a lot of computing power, so instead, I decided that if you know just one word that comes before, then you can probably guess the next word alright. I could have used all the words, but I wanted this to be fast, not useful. That, and my one-hot encoding scheme was far from optimal.</p>
<p>The same worked for logistic regression, if you care about those. It was essentially the same as the linear regression.</p>
<h3 id="Decisions-Decisions"><a href="#Decisions-Decisions" class="headerlink" title="Decisions Decisions"></a>Decisions Decisions</h3><p>Decision trees are another ML model that are as simple as they sound. You have some decision you need to make, so you ask yes&#x2F;no questions until you arrive at an answer. For example, you might have a decision tree for determining if you need an umbrella. It may ask, “is it raining?” and if you say yes, then it will say “yes, you need an umbrella.” That’s it. That’s all a decision tree is. And somehow, it generates language.</p>
<p>Rather than predicting entire words, this time, I decided to predict one letter at a time, so the decision tree is just trying to decide which letter to write next. I give it a few of the letters that come before (not even a whole word) and it asks questions like “is the first letter an ‘a’” and uses that to figure out which letter to write. The math for this was more intense than I expected, partly because I designed inefficient algorithms and my own version of formal logic to make the trees more interesting, but they work, not only writing actual words but coherent phrases.</p>
<p>You can also put together multiple trees and make a forest, just don’t forget to see the forest for the trees.</p>
<h3 id="Howdy-Neighbor"><a href="#Howdy-Neighbor" class="headerlink" title="Howdy, Neighbor"></a>Howdy, Neighbor</h3><p>In ML, cheating off your neighbor in class is encouraged. In fact, they designed a whole model off that premise. It’s called k-Nearest Neighbors, or kNN. With this model, you use the training data directly, and every time you make a prediction, you just ask, “who am I closest to, and what answer did they get?” Then, you write their answer as your own. That’s it. That’s the whole model.</p>
<p>Here, I basically treated language like a multiple choice test where you need to guess the next letter. Using this, our model could cheat off its neighbors and, like all the other models, generate text.</p>
<h3 id="Chains"><a href="#Chains" class="headerlink" title="Chains"></a>Chains</h3><p>I’ve done Markov chain text generators to death, so if you care about this one, just check out one of my other projects on the subject. <a href="https://ronikbhaskar.github.io/text_generator/">fun website</a> <a href="https://ronikbhaskar.github.io/projects/text_generator">brief summary</a></p>
<h3 id="Oh-You-Poor-Naive-Bayes"><a href="#Oh-You-Poor-Naive-Bayes" class="headerlink" title="Oh, You Poor, Naive Bayes"></a>Oh, You Poor, Naive Bayes</h3><p>Naive Bayes is just a dumb version of a very powerful statistical model. It’s naive because you assume there is no bias in what data you might ever see, ever, and that all the data is completely unrelated, somehow. You shouldn’t assume these things, but I did, because it makes for a very simple ML model. Like the linear regression, it tries to predict the next word, but this time, it does so by checking each possible word and asking, “does this word usually come after the one I’m looking at?” and “is this word rare or common?”</p>
<p>By combining those two questions, it figures out which word is most likely. Surprisingly, this model tends to use uncommon words more often than the other models.</p>
<h2 id="The-Proof-of-the-Pudding-is-in-the-Eating"><a href="#The-Proof-of-the-Pudding-is-in-the-Eating" class="headerlink" title="The Proof of the Pudding is in the Eating"></a>The Proof of the Pudding is in the Eating</h2><p>I wouldn’t be mad if you if you didn’t believe me. I mean, I just said that I made text generators from some very absurd ideas, like drawing lines and cheating on a test. So, to assuage any concerns you may have, I tested all of our models on their descriptions, and here is what they had to say:</p>
<p>Linear Regression:</p>
<blockquote>
<p>Be fast, right. I also needed to be fast, and a trend line drawn through those points, then you can use this to change our assumptions. I wanted this to generate text, not useful. It makes sense. To change our assumptions. You know just drawing lines through those points scattered on a bunch of points scattered on a plot. Earlier, I essentially the same as it turns out, I also needed to generate text, I essentially turned all the words before, all the most basic ml in</p>
</blockquote>
<p>Logistic Regression:</p>
<blockquote>
<p>Let’s start with the words requires a plot, I also needed to make this to make this to change our assumptions. I also needed to change our assumptions. I also needed to make this to generate text, then you can predict the words into points scattered on a plot, congrats. You know all the words requires a plot. I could have used all the words before, I decided that if you know just drawing lines through those points scattered on a plot. If you can generate text, I also needed to</p>
</blockquote>
<p>Decision Tree:</p>
<blockquote>
<p>Decision you arrive at an answer. For example, you say yes, then it will say “yes, this time, I designed inefficient algorithms and make a forest for determining?” and my own version trees are as the decided to predicting actual words but coherent phrases. You can also put together ML model that to figure out which letter to writing actual word) and if you need an umbrella. It may ask, “is it raining?” and uses that come before (not even a whole words, this was more interesting, but coherent phrases.</p>
</blockquote>
<p>k-Nearest Neighbors:</p>
<blockquote>
<p>In ML, cheating off your own. That’s it. That’s the whole model off its neighbors and, like a multiple choice test where you use their answer as your own. That’s it. That’s it. That’s the next letter. Using this model, you need to guess the training data directly, and what answer did they designed a whole models, generate text.eir answer did they designed a whole models, generate text.letter. Using this, our model. Here, I basically treated language like a multiple choice test where you just ask, “wh</p>
</blockquote>
<p>Naive Bayes:</p>
<blockquote>
<p>Often than the one I’m looking at. Like the one I’m looking at. Surprisingly, somehow. Like the next word usually come after the one I’m looking at. Like the linear regression, but I did, but I did, ever see, somehow. Like the data is completely unrelated, does so by checking each possible word rare or common. Like the next word usually come after the next word rare or common. It’s naive because you might ever see, but I did, somehow. Surprisingly, somehow. Surprisingly</p>
</blockquote>
<h3 id="Authors"><a href="#Authors" class="headerlink" title="Authors"></a>Authors</h3><p>Jon Ma and Ronik Bhaskar</p>
<h3 id="Notes"><a href="#Notes" class="headerlink" title="Notes"></a>Notes</h3><p>*technically, they predict the next token, not word, but for our purposes, they are functionally the same.</p>

</div> 
<link rel="icon" href="favicon.ico">
<script>
    try {
        window.onload = detectors();
    } catch (error) { // doesn't seem to pose an issue when nothing happens
        console.log(error);
    }
    
</script>
  
</article>
        </div>
      </div>
      
      <div class="footer">
    <div class="flex-container">
        <div class="footer-text">
            
            
                © Ronik Bhaskar, 2022 - 2025  
            
                
        </div>
    </div>
</div>

    </div>

    
      <div class="search-popup">
    <div class="search-popup-overlay">  
    </div>
    <div class="search-popup-window" >
        <div class="search-header">
            <div class="search-input-container">
              <input autocomplete="off" autocapitalize="off" maxlength="80"
                     placeholder="Search Anything" spellcheck="false"
                     type="search" class="search-input">
            </div>
            <div class="search-close-btn">
                <div class="icon close-btn"></div>
            </div>
        </div>
        <div class="search-result-container">
        </div>
    </div>
</div>

<script>
    const searchConfig = {
        path             : "/search.xml",
        top_n_per_article: "1",
        unescape         : "false",
        trigger: "auto",
        preload: "false"
    }
</script>
<script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.4.0/dist/search.js"></script>
<script src="/js/search.js"></script>
    
    

  </body>
</html>
