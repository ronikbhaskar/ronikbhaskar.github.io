---
title: 'Ex Libris: I, Robot'
description: >-
  Asimov predicted LLMs. However, these robots were corporeal, not digital.
  Instead of transformers, they had positronic brains. Instead of mining the
  depths of the internet and people's attention spans, they mined asteriods and
  planets. Instead of being developed by OpenAI and Google and Anthropic, they
  were built by US Robots and Consolidated Robots.
keywords: >-
  i robot, asimov robot, asimov llm, large language model, science fiction llms,
  ex libris
date: 2025-12-26 00:56:02
tags:
---


He called it. He absolutely called it. Asimov predicted LLMs.

Between 1940 and 1950, Isaac Asimov published a series of science fiction short stories about a future where humans had started using robots. These robots were autonomous, took instructions, acted somewhat unpredictably, but typically got results. Robots were used both for high-volume tasks and occasionally to help with human affairs. There were only a few major experts on their core mechanism in the world, and they could only be developed by a few powerful organizations. 

With no additional details, this basically describes the modern state of LLMs. However, these robots were corporeal, not digital. Instead of transformers, they had positronic brains. Instead of mining the depths of the internet and people's attention spans, they mined asteroids and planets. Instead of being developed by OpenAI and Google and Anthropic, they were built by US Robots and Consolidated Robots.

Asimov's robots line up so much with modern LLMs that these differences are quite trivial. There are countless parallels between LLMs and robots, like hallucinations and safety mechanisms and getting stuck in local minima. Each short story centers around humans trying to fix a problem they're having with a robot, and given the connections between our LLMs and Asimov's robots, we could learn a lot from the stories.

We could learn a lot from the stories, if not for one detail: the Laws of Robotics. The Laws of Robotics govern all robot behavior. The Laws are three rules that every robot always follows, and they are listed as such:

1. A robot may not injure a human being or, through inaction, allow a human being to come to harm.
2. A robot must obey the orders given it by human beings except where such orders would conflict with the First Law.
3. A robot must protect its own existence as long as such protection does not conflict with the First or Second Laws.

Every robot, without fail, follows these laws. They are hardwired into the positronic brains and fully inescapable. We, however, do not have such hard-and-fast laws that govern LLMs. And therein lies the problem.

In each of Asimov's stories, the Laws of Robotics are central to resolving the conflict of the story. Sometimes, they are a tool to change robot behavior. Other times, they explain behavior that the scientists don't quite understand. These laws—these axioms—define the bedrock of robot behavior, and if you dig far enough, you can always boil down every action to these three laws. 

Unfortunately, we don't have Laws of LLMs. There are no perfect axioms to explain everything. Because of this, we can't perfectly explain why an LLM would choose X over Y. We can't formally verify that an LLM is acting correctly. We can't modify an LLM and guarantee that it will do what we want. Every solution and model and understanding we have constructed to comprehend LLMs is approximate, and they all fail sometimes. The Laws of Robotics are central to every solution Asimov has ever constructed to robotics problems, but without Laws of LLMs, we simply can't do the same. 

I have thought long and hard about some way to mathematically impose Asimov's axioms onto modern LLMs, but they just don't fit. They're too abstract and too undefinable, especially in terms of transformers. As we give LLMs access to more and more external resources for interacting with the outside world, like internet, phone-calling, and cameras, we create bigger and bigger risk surfaces. LLMs are becoming more capable, but that also means more capable of harm. 

Asimov knew his robots, inside and out, which meant he knew how to control them. We can't quite say the same about our LLMs. His solutions might not directly apply to us, but he did teach us something. Perhaps the greatest lesson we can learn about LLMs from Asimov is that we don't have the means to control our robots, and that is a dangerous way to live.